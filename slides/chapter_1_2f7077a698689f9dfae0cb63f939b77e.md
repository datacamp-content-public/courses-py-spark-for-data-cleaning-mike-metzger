---
title: Insert title here
key: 2f7077a698689f9dfae0cb63f939b77e

---
## Introduction to Data Pipelines

```yaml
type: "TitleSlide"
key: "d9eb6a41f4"
```

`@lower_third`

name: Mike Metzger
title: Instructor


`@script`
Now that we’ve worked through the basics of data import and manipulation in Spark, let’s talk about Data Pipelines.


---
## What is a Data Pipeline?

```yaml
type: "FullSlide"
key: "22f599cd2b"
```

`@part1`
- A set of steps to process data from source(s) to final output{{1}}
- Can consist of any number of steps or components{{2}}
- Can span many systems{{3}}
- We will focus on data pipelines within Spark{{4}}


`@script`
- Data Pipelines are simply the set of steps needed to move from an input data source, or sources, and convert it to the desired output.

- A data pipeline can consist of any number of steps or components, and can span many systems

- For our purposes, we’re going to look at setting up a data pipeline within Spark, but realize that a full production data pipeline will likely communicate with many systems when complete


---
## What does a Data Pipeline look like?

```yaml
type: "TwoColumns"
key: "d0a9717c10"
```

`@part1`
Input -> Processing -> Output


`@part2`
Read -> Transform -> Actions


`@script`
When we consider the components for a basic data pipeline, we need to think about what we start with, or our input files.  In this course, we’ve discussed how to read CSV, text, and JSON files. We could also read data directly from web services, APIs, databases, etc.  All of these sources can be read into a Spark DataFrame, the primary data structure for our pipeline.

Once we have the data in a dataframe, we need to transform it in some fashion.  You’ve done the individual steps several times throughout this course - adding columns, filtering rows, performing aggregate calculations as neeeded.  In our previous examples, we’ve only done one of these steps at a time but a pipeline can consist of as many of these steps as we’d like so we can format the data into our desired output.

Finally, once we’ve defined our transformations we need to output the data into a useable form.  Again, you’ve already written files out to CSV or Parquet format, but it could include multiple copies with various formats, or instead write the output to a database, a web service, and so forth.


---
## Example Spark Data Pipeline

```yaml
type: "TwoColumns"
key: "40adc668d5"
```

`@part1`
- Date{{1}}
- Departure City{{2}}
- Arrival City{{3}}
- Flight Number{{4}}
- Duration{{5}}


`@part2`
```
# Create the Schema
FlightRecordSchema = StructType([ \
	StructField(“departure_city”, StringType(), True), \
	StructField(“arrival_city”, StringType(), True), \
	StructField(“flight_time”, IntegerType(), True) ])
```


`@script`
Now let’s take a look at an example Data Pipeline in Spark.

Assuming we’ve imported our necessary modules, let’s look at a set of flight data.

Our flight data is pretty simple in this case, consisting of a date, departure and arrival cities, a flight number, and the duration of the flight.

  As we’ve discussed previously, we’ll want to define a schema


---
## Final Slide

```yaml
type: "FinalSlide"
key: "dfb9120c4d"
```

`@script`


