---
title: Insert title here
key: 2f7077a698689f9dfae0cb63f939b77e
video_link:
  mp3: https://assets.datacamp.com/production/repositories/4284/datasets/1e76f970eb3873617dc4b9e069941acdeddb6cd6/IntroToDataPipelines.mp3

---
## Introduction to Data Pipelines

```yaml
type: "TitleSlide"
key: "d9eb6a41f4"
```

`@lower_third`

name: Mike Metzger
title: Data Engineer


`@script`
Now that we’ve worked through the basics of data import and manipulation in Spark, let’s talk about Data Pipelines.


---
## What is a Data Pipeline?

```yaml
type: "FullSlide"
key: "22f599cd2b"
```

`@part1`
- A set of steps to process data from source(s) to final output{{1}}
- Can consist of any number of steps or components{{2}}
- Can span many systems{{3}}
- We will focus on data pipelines within Spark{{4}}


`@script`
- Data Pipelines are simply the set of steps needed to move from an input data source, or sources, and convert it to the desired output.

- A data pipeline can consist of any number of steps or components, and can span many systems

- For our purposes, we’ll be setting up a data pipeline within Spark, but realize that a full production data pipeline will likely communicate with many systems.


---
## What does a Data Pipeline look like?

```yaml
type: "FullSlide"
key: "97b50e8fed"
```

`@part1`
- Input(s){{1}}
- Transformations{{2}}
- Output(s){{3}}


`@script`
When we consider the components for a basic data pipeline, we need to think about what we start with, or our input files.  In this course, we’ve discussed how to read CSV, text, and JSON files. We could also read data directly from web services, APIs, databases, etc.  All of these sources can be read into a Spark DataFrame, the primary data structure for our pipeline.

Once we have the data in a dataframe, we need to transform it in some fashion.  You’ve done the individual steps several times throughout this course - adding columns, filtering rows, performing aggregate calculations as needed.  In our previous examples, we’ve only done one of these steps at a time but a pipeline can consist of as many of these steps as we’d like so we can format the data into our desired output.

Finally, once we’ve defined our transformations we need to output the data into a usable form.  You’ve already written files out to CSV or Parquet format, but it could include multiple copies with various formats, or instead write the output to a database, a web service, and so forth.


---
## Example Spark Data Pipeline

```yaml
type: "FullCodeSlide"
key: "a1f9ad7be7"
```

`@part1`
Create the Schema{{1}}
```
FlightRecordSchema = StructType([ 
    StructField(“str_date”,StringType(),False), 
    StructField(“flight_number”, StringType(), False), 
    StructField(“arrival_city”, StringType(), False), 
    StructField("str_elapsed_time", StringType(), False ])
```{{2}}
Define the DataFrame{{3}}
```
FlightRecords_df = spark.read.format('csv')
    .options(header='true',inferSchema='false')
    .schema(FlightRecordSchema)
    .load('AA_DFW_*_Departures_Short.csv')
```{{4}}


`@script`
Now let’s take a look at an example Data Pipeline in Spark.

Assuming we’ve imported our necessary modules, let’s look at a set of flight data.

Our flight data is pretty simple, consisting of a date, flight number,  arrival cities, and the duration of the flight.

  We’ll want to define a schema for the data. This speeds up import which is important on large data sets. It also lets Spark drop rows with incorrect data during import.

For this example, we're going to define our DataFrame options, give it the schema we've created, and load the data.


---
## Example Spark Data Pipeline - Transformations

```yaml
type: "FullCodeSlide"
key: "29e9de560a"
```

`@part1`
Convert Column{{1}}
```
FlightRecords_df = FlightRecords_df.
    withColumn('date', to_date(FlightRecords_df.str_date, 'MM/dd/yyyy'))
```{{2}}
Drop Column{{3}}
```
FlightRecords_df = FlightRecords_df.drop(FlightRecords_df.str_date)
```{{4}}
Filter Data{{5}}
```
FlightRecords_df = FlightRecords_df
    .filter(FlightRecords_df.elapsed_time > 0)
```{{6}}


`@script`
Once our DataFrame is defined, we need to perform some transformations to prepare the data.  

First, we're going to convert the str_date column from a string to a date. This will let us perform richer queries on the data vs string form.

Next, let's drop the original str_date column since we won't be using it.

We've noticed that there are some rows with an elapsed time of zero. This information could negatively affect any aggregate calculations we may perform later.  Let's filter those out now by only including rows where the elapsed_time is greater than zero.


---
## Example Spark Data Pipeline - Continued

```yaml
type: "FullCodeSlide"
key: "1f78f7a168"
```

`@part1`
Add an ID column
```
FlightRecords_df = FlightRecords_df
    .withColumn('flight_id', monotonically_increasing_id())
```


`@script`
The last transformation we want to do is to add a column for a flight id.  We'll use the withColumn method and the monotonically_increasing_id function to add an ID automatically.


---
## Example Spark Data Pipeline - Export

```yaml
type: "FullCodeSlide"
key: "b63a553924"
```

`@part1`
Saving data in Parquet format
```
FlightRecords_df.write.parquet('FlightRecords_Cleaned_2014-2018.parquet')
```


`@script`
Finally, let's save our data in Parquet format so we can query it later without having to recreate this data set.


---
## Let's Practice!

```yaml
type: "FinalSlide"
key: "dfb9120c4d"
```

`@script`
When you look at the components, a data pipeline in Spark is fairly simple, but can be very powerful.

You've just seen a multi-step pipeline - now it's time to build your own in the exercises ahead.

